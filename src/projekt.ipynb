{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualization of annual weather measurements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cols\n",
    "import pandas as pd\n",
    "import json\n",
    "import transform_in_spark\n",
    "from hdfs import InsecureClient\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import StructType,StructField, StringType, IntegerType, FloatType, DateType\n",
    "import aggregate\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "local_data_path = '../Data/'\n",
    "\n",
    "pivot_start_year = 1900\n",
    "pivot_end_year = 1929\n",
    "\n",
    "# HDFS\n",
    "hdfs_url = 'http://localhost:9870'\n",
    "hdfs_user = 'my_username'\n",
    "hdfs_dir = '/bigdata/'\n",
    "datalake_dir = hdfs_dir + 'datalake/'\n",
    "data_temp_prepared = 'data_temp_prepared.csv'\n",
    "data_stations_prepared = 'data_stations_prepared.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "hdfs_client = InsecureClient(hdfs_url, user=hdfs_user)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder \\\n",
    "      .master(\"yarn\") \\\n",
    "      .appName(\"weather_data\") \\\n",
    "      .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Acquire data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data are put into a Hadoop datalake. For simplicity, we downloaded the data beforehand and stored it on a local drive. From ther, we copy the data into the datalake.\n",
    "\n",
    "- Measurements of temperature, rainfall and data on stations: Annual means provided by the Deutscher Wetterdienst, https://cdc.dwd.de/portal/\n",
    "- GeoJSONs: GeoJSONs in lowest resolution, taken from https://github.com/isellsoap/deutschlandGeoJSON/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "datapath = Path(local_data_path).absolute()\n",
    "\n",
    "def put_to_hdfs(local_path, hdfs_path):\n",
    "    try:\n",
    "        hdfs_client.upload(hdfs_path, local_path)\n",
    "        print(f\"Copied file to HDFS successfully: {Path(local_path).name}\")\n",
    "    except Exception as e:\n",
    "        print(\"Error while copying the file to HDFS: \", e)\n",
    "\n",
    "\n",
    "print('starting data acquisition')\n",
    "for path in datapath.iterdir():\n",
    "    if path.is_file():\n",
    "        if path.suffix == '.csv':\n",
    "            name = path.name\n",
    "            if name.startswith('temperatures_') or name.startswith('stations_') or name.startswith('rainfall_'):\n",
    "                put_to_hdfs( local_data_path + name, datalake_dir + name )\n",
    "print('data acquisition complete')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "## Read data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements of temperature and rainfall contain one measurement per line, a timestamp and an ID of the station. Alongside, there are some meta data of the station, too.\n",
    "\n",
    "As main data, the station data contain an ID and the geolocation of each station.\n",
    "\n",
    "The GeoJSONs are JSONs in a special format. They list data on each area listed in a file. Main infos for our needs are the name of the areas and polygons representing the area boundaries. The GeoJSON of the counties also include the names of the larger areas the county is located in, i.e. state and nation.\n",
    "\n",
    "The choropleth map requires these GeoJSONs for displaying the areas. As a handy plus, we can use the GeoJSON of the counties for assigning the stations to the areas they are located in. The geolocations of the stations and the polygons in the GeoJSON are used for this automated assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read measurements of temperatues and restrict to the columns we need\n",
    "schema_temps = StructType([\n",
    "    StructField( 'Produkt_Code', StringType(), True),\n",
    "    StructField( cols.sdo_id, StringType(), True),\n",
    "    StructField( cols.timestamp, DateType(), True),\n",
    "    StructField( cols.temperature, FloatType(), True),\n",
    "    StructField( 'Qualitaet_Byte', IntegerType(), True),\n",
    "    StructField( 'Qualitaet_Niveau', IntegerType(), True),\n",
    "])\n",
    "\n",
    "sdf_temps = spark.read.option('header', True) \\\n",
    "        .schema(schema_temps) \\\n",
    "        .csv('hdfs://'+datalake_dir+'temperatures_*.csv') \\\n",
    "        .select(cols.sdo_id, cols.timestamp, cols.temperature) \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# Read measurements of rainfall and restrict to the columns we need\n",
    "schema_rainfall = StructType([\n",
    "    StructField( 'Produkt_Code', StringType(), True),\n",
    "    StructField( cols.sdo_id, StringType(), True),\n",
    "    StructField( cols.timestamp, DateType(), True),\n",
    "    StructField( cols.rainfall, FloatType(), True),\n",
    "    StructField( 'Qualitaet_Byte', IntegerType(), True),\n",
    "    StructField( 'Qualitaet_Niveau', IntegerType(), True),\n",
    "])\n",
    "\n",
    "sdf_rainfall = spark.read.option('header', True) \\\n",
    "        .schema(schema_rainfall) \\\n",
    "        .csv('hdfs://'+datalake_dir+'rainfall_*.csv') \\\n",
    "        .select(cols.sdo_id, cols.timestamp, cols.rainfall) \\\n",
    "        .drop_duplicates()\n",
    "\n",
    "\n",
    "\n",
    "# Read data of stations and restrict to the columns we need\n",
    "# CAUTION: cols.longitude and cols.latitude use ',' as period, while cols.altitude uses '.'\n",
    "schema_stations = StructType([\n",
    "    StructField( cols.sdo_id, StringType(), True ),\n",
    "    StructField( 'SDO_Name', StringType(), True ),\n",
    "    StructField( cols.longitude, StringType(), True ),\n",
    "    StructField( cols.latitude, StringType(), True ),\n",
    "    StructField( cols.altitude, FloatType(), True ),\n",
    "    StructField( 'Metadata_Link', StringType(), True ),\n",
    "])\n",
    "\n",
    "sdf_stations_0 = spark.read.option('header', True) \\\n",
    "        .schema(schema_stations).csv('hdfs://'+datalake_dir+'stations_*.csv') \\\n",
    "        .select(cols.sdo_id, 'SDO_Name', cols.longitude, cols.latitude) \\\n",
    "        .dropDuplicates([cols.sdo_id]) \\\n",
    "        .withColumn( cols.longitude, regexp_replace( col(cols.longitude), ',', '.').cast('float') ) \\\n",
    "        .withColumn( cols.latitude, regexp_replace( col(cols.latitude), ',', '.').cast('float') )\n",
    "\n",
    "sdf_temps.printSchema()\n",
    "sdf_temps.show()\n",
    "sdf_rainfall.printSchema()\n",
    "sdf_rainfall.show()\n",
    "sdf_stations_0.printSchema()\n",
    "sdf_stations_0.show()\n",
    "\n",
    "# GeoJSONs\n",
    "# highest level\n",
    "with open( local_data_path + '4_niedrig_bund.geo.json' ) as file:\n",
    "    national_json = json.load( file )\n",
    "\n",
    "# first level of subdivisions\n",
    "with open( local_data_path + '4_niedrig_laender.geo.json' ) as file:\n",
    "    state_json = json.load( file )\n",
    "\n",
    "# second level of subdivisions\n",
    "with open( local_data_path + '4_niedrig_kreise.geo.json' ) as file:\n",
    "    county_json = json.load( file )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The measurements contain a timestamp. We extract the year and add this as a new column.\n",
    "\n",
    "Also, we compute an individual reference temperature for each station by determining the mean temperature in a certain, userdefined time interval. Note that this ends up in *NULL* for stations without measurements in this interval."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add numeric column for the year; Add numeric column for reference temperature\n",
    "sdf_temps = sdf_temps.withColumn( cols.year, year(cols.timestamp) )\n",
    "sdf_temps = sdf_temps.join(\n",
    "            aggregate.get_pivotal_mean( sdf_temps, pivot_start_year, pivot_end_year ),\n",
    "            on = cols.sdo_id,\n",
    "            how = 'left'\n",
    "        )\n",
    "\n",
    "# Add numeric column for the year\n",
    "sdf_rainfall = sdf_rainfall.withColumn( cols.year, year(cols.timestamp) )\n",
    "\n",
    "# just for the show() we shuffle the lines. Otherwise, we would see just the first 20 years of the same station.\n",
    "sdf_temps.printSchema()\n",
    "sdf_temps.withColumn( 'rand', (rand(seed=123456789)*100000).cast('int') ).sort( 'rand' ).drop('rand').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We expand the station data by the nations, the states, and the counties each station is located in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register required modules on the nodes of the cluster\n",
    "# Make sure the module 'shapely' is available on each node of the cluster\n",
    "spark.sparkContext.addPyFile('./cols.py')\n",
    "spark.sparkContext.addPyFile('./transform_in_spark.py')\n",
    "\n",
    "aux_col_areas = 'areas' # name of auxilliary column\n",
    "udf_find_area = udf(\n",
    "    lambda lon, lat: transform_in_spark.find_area( lon, lat, county_json ),\n",
    "    StructType([\n",
    "        StructField( 'Nation', StringType(), True ),\n",
    "        StructField( 'State', StringType(), True ),\n",
    "        StructField( 'County', StringType(), True ),\n",
    "        StructField( 'Assignment', StringType(), True ),\n",
    "    ])\n",
    ")\n",
    "\n",
    "# flatten the auxillary, nested column and drop it\n",
    "sdf_stations = sdf_stations_0.withColumn( aux_col_areas, udf_find_area( col(cols.longitude), col(cols.latitude) ) ) \\\n",
    "        .withColumn( cols.nation, col('areas.Nation') ) \\\n",
    "        .withColumn( cols.state, col('areas.State') ) \\\n",
    "        .withColumn( cols.county, col('areas.County') ) \\\n",
    "        .withColumn( cols.assignment, col('areas.Assignment') ) \\\n",
    "        .drop( aux_col_areas )\n",
    "\n",
    "sdf_stations.printSchema()\n",
    "sdf_stations.withColumn( 'rand', (rand(seed=5)*10000000).cast('int') ).sort( 'rand' ).drop('rand').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Caution!** The polygons in the GeoJSONs just approximate the boundaries of the political areas. Especially stations close to the borders have a fair chance to be located outside of the approximating polygon. These may not be assigned to any area, or might be even assigned to a wrong area. If we do not correct the processed data for these effects, we have to expects *NULLs* at least."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For stations where the automated assignment to the areas has failed, this can be done manually.\n",
    "# As an example, we do this here for few stations, outlining the way\n",
    "\n",
    "sdf_stations.where( col(cols.nation).isNull() | col(cols.state).isNull() | col(cols.county).isNull() ).show()\n",
    "\n",
    "def set_area(sdo_id: str, county: str, state: str, nation: str = 'Deutschland'):\n",
    "    aux = sdf_stations.withColumn( cols.nation, when( col(cols.sdo_id) == sdo_id, nation ).otherwise( col(cols.nation) ) ) \\\n",
    "            .withColumn( cols.state, when( col(cols.sdo_id) == sdo_id, state).otherwise( col(cols.state )) ) \\\n",
    "            .withColumn( cols.county, when( col(cols.sdo_id) == sdo_id, county).otherwise( col(cols.county) ) ) \\\n",
    "            .withColumn( cols.assignment, when( col(cols.sdo_id) == sdo_id, cols.manually).otherwise( col(cols.assignment) ) )\n",
    "    return aux\n",
    "\n",
    "sdf_stations = set_area( '1014', 'Leer', 'Niedersachsen' )\n",
    "sdf_stations = set_area( '1260', 'Bernkastel-Wittlich', 'Rheinland-Pfalz')\n",
    "sdf_stations = set_area( '13714', 'Neuwied', 'Rheinland-Pfalz')\n",
    "\n",
    "sdf_stations.where( (col(cols.sdo_id) == '1014') | (col(cols.sdo_id) == '1260') | (col(cols.sdo_id) == '13714') ).show()\n",
    "print('area not assigned for', sdf_stations.filter( col(cols.nation).isNull() | col(cols.state).isNull() | col(cols.county).isNull() ).count(), 'out of', sdf_stations.count() )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a complete list with the name of all areas. There will be areas without data. These would be dropped on the way when alysing the data. But we want these areas to be included in the analysis results, with *NULLs* as values, or with 0 for the number of stations, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# extract names of all areas for each level of detail\n",
    "national_names = [national_json['features'][idx]['properties']['NAME_LOCAL'] for idx in range(len(national_json['features']))]\n",
    "state_names = [state_json['features'][idx]['properties']['name'] for idx in range(len(state_json['features']))]\n",
    "county_names = [county_json['features'][idx]['properties']['NAME_3'] for idx in range(len(county_json['features']))]\n",
    "\n",
    "# convert python arrays to spark DataFrames\n",
    "schema = StructType([ StructField( cols.area, StringType(), False ) ])\n",
    "sdf_national_names = spark.createDataFrame( [(area,) for area in national_names], schema )\n",
    "\n",
    "schema = StructType([ StructField( cols.area, StringType(), False ) ])\n",
    "sdf_state_names = spark.createDataFrame( [(area,) for area in state_names], schema )\n",
    "\n",
    "schema = StructType([ StructField( cols.area, StringType(), False ) ])\n",
    "sdf_county_names = spark.createDataFrame( [(area,) for area in county_names], schema )\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create spark data frame for data analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sdf = sdf_temps.join( sdf_rainfall, on = [cols.sdo_id, cols.year], how = 'outer' )\n",
    "sdf = sdf.join( sdf_stations, on = cols.sdo_id, how = 'left' ).drop( cols.timestamp )\n",
    "sdf.printSchema()\n",
    "sdf.select([cols.sdo_id, 'SDO_Name', cols.year, cols.pivot_temp,cols.temperature,cols.rainfall,cols.nation,cols.state,cols.county]) \\\n",
    "        .withColumn( 'rand', (rand(seed=5647382910)*10000000).cast('int') ).sort( 'rand' ).drop('rand').show()\n",
    "\n",
    "print(\n",
    "    'stations with temperatures recorded but no area assigned:',\n",
    "    sdf.where( col(cols.temperature).isNotNull() ).filter( col(cols.nation).isNull() | col(cols.state).isNull() | col(cols.county).isNull() ).dropDuplicates([cols.sdo_id]).count(),\n",
    "    'out of',\n",
    "    sdf.where( col(cols.temperature).isNotNull() ).dropDuplicates([cols.sdo_id]).count()\n",
    ")\n",
    "\n",
    "# esingle colums data frame with all years of measurements. This is used for the x axis of our plots later on.\n",
    "year_min = sdf.agg( min(cols.year) ).collect()[0][0]\n",
    "year_max = sdf.agg( max(cols.year) ).collect()[0][0]\n",
    "\n",
    "schema = StructType([ StructField( cols.year, IntegerType(), False ) ])\n",
    "sdf_years = spark.createDataFrame( [(_,) for _ in range(year_min, year_max+1)], schema )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualizing with Plotly Dash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dash import Dash, html, dcc, callback, Input, Output, ctx\n",
    "from typing import List\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "\n",
    "# --------  create choropleth figures ----------\n",
    "\n",
    "def make_choropleth_figure(df: pd.DataFrame, use_json, key_id: str, min_col: int, max_col: int):\n",
    "    \"\"\"\n",
    "    Creates the plotly express choropleth map.\n",
    "\n",
    "    ---\n",
    "    df: pandas data frame with columns 'name' (str) and 'funniness' (int).\n",
    "\n",
    "    use_json: GeoJSON to be used.\n",
    "\n",
    "    key_id: where the name of an area can be found in the GeoJSON.\n",
    "\n",
    "    min_col:\n",
    "    Bottom of colorscale corresponds to this vale.\n",
    "\n",
    "    max_col: int\n",
    "    Top of color scale corresponds to this value.\n",
    "    ---\n",
    "    returns: figure\n",
    "    \"\"\"\n",
    "    fig = px.choropleth(df,\n",
    "                    geojson=use_json,\n",
    "                    locations=cols.area,\n",
    "                    featureidkey=key_id,\n",
    "                    color=cols.mean,\n",
    "                    color_continuous_scale='Viridis',\n",
    "                    range_color=(min_col, max_col),\n",
    "                    labels={cols.mean,'Value'},\n",
    "                    hover_name=cols.area,\n",
    "                    hover_data=[cols.mean, cols.stddev, cols.stations]\n",
    "    )\n",
    "    # Disable underlying outline of global land masses and fit view to the bounds of the displayed area\n",
    "    fig.update_geos(\n",
    "        visible=False,\n",
    "        fitbounds='locations'\n",
    "    )\n",
    "    fig.update_layout(margin={'r':0,'t':0,'l':0,'b':0})\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# ---------  create plots of temperature over timer  ----------\n",
    "\n",
    "def make_temperature_plot(pdf_annual: pd.DataFrame, pdf_running: pd.DataFrame, title: str, y_label: str):\n",
    "    \"\"\"\n",
    "    Makes a plot with two curves.\n",
    "\n",
    "    ---\n",
    "    pdf_annual: pd.DataFrame\n",
    "    DataFrame with actual annual values.\n",
    "\n",
    "    pdf_running: pd.DataFrame\n",
    "    DataFrame containing running means for each year.\n",
    "\n",
    "    title: str\n",
    "    Title of the plot\n",
    "    \"\"\"\n",
    "    fig = go.Figure()\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = pdf_annual[cols.year],\n",
    "            y = pdf_annual[cols.mean],\n",
    "            name = 'Annual mean'\n",
    "        )\n",
    "    )\n",
    "    fig.add_trace(\n",
    "        go.Scatter(\n",
    "            x = pdf_running[cols.year],\n",
    "            y = pdf_running[cols.running],\n",
    "            name = '7-year running mean'\n",
    "        )\n",
    "    )\n",
    "\n",
    "\n",
    "    # add labels\n",
    "    fig.update_layout(\n",
    "        title = {'text': title},\n",
    "        xaxis = { 'title': {'text': 'Year'}},\n",
    "        yaxis = { 'title': {'text': y_label}},\n",
    "    )\n",
    "    fig.update_traces(mode='lines')\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "def make_temp_diff_plot(area_col_name, area):\n",
    "    pdf = sdf.where( col(area_col_name) == area ) \\\n",
    "            .where( col(cols.temperature).isNotNull() ) \\\n",
    "            .where( col(cols.pivot_temp).isNotNull() ) \\\n",
    "            .withColumn( 'delta', col(cols.temperature) - col(cols.pivot_temp) ) \\\n",
    "            .groupBy( cols.year ) \\\n",
    "            .agg(\n",
    "                mean( 'delta' ).alias( cols.mean ),\n",
    "                stddev( 'delta' ).alias( cols.stddev ),\n",
    "                count( 'delta' ).alias( cols.stations )\n",
    "            ) \\\n",
    "            .join( sdf_years, on=cols.year, how='outer' ) \\\n",
    "            .fillna( 0, cols.stations ) \\\n",
    "            .sort( cols.year, ascending=True ) \\\n",
    "            .toPandas()\n",
    "\n",
    "    fig = px.scatter( pdf, x=cols.year, y=cols.mean, error_y=cols.stddev, hover_name=cols.year, hover_data=[cols.stations])\n",
    "\n",
    "    # add labels\n",
    "    fig.update_layout(\n",
    "        title = {'text': f'Annual temperature anomaly in {area} compared to the station mean temperatures in years {pivot_start_year} through {pivot_end_year}'},\n",
    "        xaxis = { 'title': {'text': 'Year'}},\n",
    "        yaxis = { 'title': {'text': 'Temperature difference [°C]'}},\n",
    "    )\n",
    "\n",
    "    return fig\n",
    "\n",
    "\n",
    "\n",
    "# ---------  UI elements with simple settings  ----------\n",
    "\n",
    "level_options = ['Nation', 'State', 'Counties']\n",
    "level_select_id = 'level_select'\n",
    "level_select = dcc.Dropdown(level_options, level_options[0], id=level_select_id)\n",
    "level_select_label = html.Label( 'Select level of detail', htmlFor=level_select_id )\n",
    "\n",
    "measurement_options = ['Temperature', 'Rainfall']\n",
    "measurement_select_id = 'measurement_select'\n",
    "measurement_select = dcc.Dropdown( measurement_options, measurement_options[0], id = measurement_select_id )\n",
    "measurement_select_label = html.Label( 'Select measurement', htmlFor=measurement_select_id )\n",
    "\n",
    "cmap = dcc.Graph()\n",
    "plot_label = html.Div()\n",
    "year_text = html.Div()\n",
    "\n",
    "\n",
    "\n",
    "# ---------  slider for selecting the year  ----------\n",
    "\n",
    "year_slider_id = 'year_slider'\n",
    "year_ticks_spacing = 10\n",
    "\n",
    "year_slider_label = html.Label('Select year', htmlFor=year_slider_id)\n",
    "year_slider = dcc.Slider(\n",
    "            min = year_min,\n",
    "            max = year_max,\n",
    "            marks={ year: str(year) for year in range(year_min, year_max+1, year_ticks_spacing) },\n",
    "            value=year_max,\n",
    "            step=1,\n",
    "            id=year_slider_id\n",
    "        )\n",
    "\n",
    "\n",
    "\n",
    "# ----------  plot of temperature  ----------\n",
    "\n",
    "plot = dcc.Graph()\n",
    "diff_plot = dcc.Graph()\n",
    "\n",
    "\n",
    "\n",
    "# ----------  layout dahsboard  ----------\n",
    "\n",
    "app = Dash()\n",
    "app.layout = html.Div([\n",
    "    level_select_label,\n",
    "    level_select,\n",
    "    measurement_select_label,\n",
    "    measurement_select,\n",
    "    cmap,\n",
    "    year_slider_label,\n",
    "    year_slider,\n",
    "    year_text,\n",
    "    plot,\n",
    "    diff_plot\n",
    "])\n",
    "\n",
    "\n",
    "\n",
    "# --------  callbacks  ---------\n",
    "\n",
    "@callback(\n",
    "    Output( cmap, 'figure' ),\n",
    "    Output( year_text, 'children' ),\n",
    "    Input( level_select, 'value' ),\n",
    "    Input( measurement_select, 'value' ),\n",
    "    Input( year_slider, 'value' ),\n",
    "    running=[\n",
    "            (Output(level_select_id, \"disabled\"), True, False),\n",
    "            (Output(measurement_select_id, \"disabled\"), True, False),\n",
    "            (Output(year_slider_id, \"disabled\"), True, False)\n",
    "        ]\n",
    ")\n",
    "def set_level_of_detail(value_lod: str, value_meas: str, year: int) -> str:\n",
    "    # get GeoJSON-specific values\n",
    "    if value_lod == level_options[2]:\n",
    "        use_geojson = county_json\n",
    "        key_id = 'properties.NAME_3'\n",
    "        area_col_name = cols.county\n",
    "        full_list = sdf_county_names\n",
    "    elif value_lod == level_options[1]:\n",
    "        use_geojson = state_json\n",
    "        key_id = 'properties.name'\n",
    "        area_col_name = cols.state\n",
    "        full_list = sdf_state_names\n",
    "    else:\n",
    "        use_geojson = national_json\n",
    "        key_id = 'properties.NAME_LOCAL'\n",
    "        area_col_name = cols.nation\n",
    "        full_list = sdf_national_names\n",
    "\n",
    "    # get name of column with measurements of interest\n",
    "    if value_meas == measurement_options[0]:\n",
    "        value_col_name = cols.temperature\n",
    "        min_col = 7\n",
    "        max_col = 12\n",
    "    else:\n",
    "        value_col_name = cols.rainfall\n",
    "        min_col = 400\n",
    "        max_col = 2000\n",
    "\n",
    "    # aggregate and plot data\n",
    "    df = aggregate.aggregate_spatially( sdf, year, value_col_name, area_col_name, full_list )\n",
    "    fig = make_choropleth_figure( df, use_geojson, key_id, min_col, max_col )\n",
    "\n",
    "    text = f'Selected year: {year}'\n",
    "    return fig, text\n",
    "\n",
    "\n",
    "\n",
    "@callback(\n",
    "    Output(plot, 'figure'),\n",
    "    Output(diff_plot, 'figure'),\n",
    "    Input( cmap, 'clickData' ),\n",
    "    Input( level_select, 'value' ),\n",
    "    Input( measurement_select, 'value' )\n",
    ")\n",
    "def show_additional_data(clickData, value_lod, value_meas):\n",
    "    caused_by = ctx.triggered_id\n",
    "    if caused_by == level_select_id or caused_by == measurement_select_id:\n",
    "        # clear additional output\n",
    "        return {}, {}\n",
    "\n",
    "    if clickData is not None:\n",
    "        area_name = clickData['points'][0]['location']\n",
    "\n",
    "        # translate values of drop downs to data-frame column-names\n",
    "        dic = {level_options[0]: cols.nation, level_options[1]: cols.state, level_options[2]: cols.county}\n",
    "        area_col_name = dic[value_lod]\n",
    "\n",
    "        # set values specific to the measurements\n",
    "        if value_meas == measurement_options[0]:\n",
    "            value_col_name = cols.temperature\n",
    "            title = f'Annual mean temperatures in {area_name}'\n",
    "            y_label = 'Temperature [°C]'\n",
    "        else:\n",
    "            value_col_name = cols.rainfall\n",
    "            title = f'Annual total rainfall in {area_name}'\n",
    "            y_label = 'Rainfall [l/m²]'\n",
    "\n",
    "        # aggregate and plot data\n",
    "        pdf_annual = aggregate.aggregate_temporarly( sdf, value_col_name, area_name, area_col_name, sdf_years )\n",
    "        pdf_running = aggregate.aggregate_running_mean( sdf, value_col_name, area_name, area_col_name, sdf_years )\n",
    "\n",
    "        temp_plot = make_temperature_plot( pdf_annual, pdf_running, title, y_label )\n",
    "        if value_meas == measurement_options[0]:\n",
    "            diff_plot = make_temp_diff_plot( area_col_name, area_name )\n",
    "        else:\n",
    "            diff_plot = {}\n",
    "\n",
    "        output = temp_plot, diff_plot\n",
    "    else:\n",
    "        output = {}, {}\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    app.run(debug=False, port=8054)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
